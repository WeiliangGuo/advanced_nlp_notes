**Bidirectional-LSTM-CRF** model used for NER tasks doesn't reply on language-specific knowledge or resources such as gazetteers. It may achieve state-of-the-art performance compared to other models such as Hidden-Markov-Model.



**References:**
* [Sequence Tagging With A LSTM-CRF](https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/)
* [Neural Architectures for Named Entity Recognition](https://arxiv.org/pdf/1603.01360.pdf)
* [Maximum Entropy Principle](https://www.youtube.com/watch?v=ynCkUHPEDOI&t=616s)
* [What is Trasition Matrix?](https://www.youtube.com/watch?v=4zg5bNlHZRg&t=20s)
* [Introduction to Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)
* [anaGo: an NER package based on Keras framework and BLSTM-CRF model](https://github.com/Hironsan/anago)
* [Andrew Ng's 5th course Sequence Models of Deep Learning](deeplearning.ai)
